left_join(m1_petasq, by = "Effect") %>%
left_join(m1_petasq_cis, by = "Effect") %>%
filter(Effect != "(Intercept)") %>%
dplyr::rename(DF1 = DFn,
DF2 = DFd) %>%
rowwise() %>%
mutate(p = apa_p_value(p),
petasq = as.numeric(as.character(petasq)),
es_interpretation = ifelse(petasq < 0.01, "negligable",
ifelse(petasq < 0.06, "small",
ifelse(petasq < 0.14, "medium", "large"))),
petasq = ifelse(petasq < 0.01, "<.01", petasq),
result = paste("F(", DF1, ", ", DF2, ") = ", F, ", p ", p, "eta2 = ", petasq, ", ", petasq_ci,sep = "")) %>%
ungroup()
m1_output
# combine all results into a single df
m1_output <- m1_summary %>%
left_join(m1_petasq, by = "Effect") %>%
left_join(m1_petasq_cis, by = "Effect") %>%
filter(Effect != "(Intercept)") %>%
dplyr::rename(DF1 = DFn,
DF2 = DFd) %>%
rowwise() %>%
mutate(p = apa_p_value(p),
petasq = as.numeric(as.character(petasq)),
es_interpretation = ifelse(petasq < 0.01, "negligable",
ifelse(petasq < 0.06, "small",
ifelse(petasq < 0.14, "medium", "large"))),
petasq = ifelse(petasq < 0.01, "<.01", petasq),
result = paste("F(", DF1, ", ", DF2, ") = ", F, ", p ", p, ", $\eta$~p~^2^ = ", petasq, ", ", petasq_ci,sep = "")) %>%
# combine all results into a single df
m1_output <- m1_summary %>%
left_join(m1_petasq, by = "Effect") %>%
left_join(m1_petasq_cis, by = "Effect") %>%
filter(Effect != "(Intercept)") %>%
dplyr::rename(DF1 = DFn,
DF2 = DFd) %>%
rowwise() %>%
mutate(p = apa_p_value(p),
petasq = as.numeric(as.character(petasq)),
es_interpretation = ifelse(petasq < 0.01, "negligable",
ifelse(petasq < 0.06, "small",
ifelse(petasq < 0.14, "medium", "large"))),
petasq = ifelse(petasq < 0.01, "<.01", petasq),
result = paste("F(", DF1, ", ", DF2, ") = ", F, ", p ", p, ", eta2 = ", petasq, ", ", petasq_ci,sep = "")) %>%
ungroup()
# results for printing
m1_iv_result <- m1_output %>%
filter(Effect == "category") %>%
select(result) %>%
as.character()
m1_iv_result
# combine all results into a single df
m1_output <- m1_summary %>%
left_join(m1_petasq, by = "Effect") %>%
left_join(m1_petasq_cis, by = "Effect") %>%
filter(Effect != "(Intercept)") %>%
dplyr::rename(DF1 = DFn,
DF2 = DFd) %>%
rowwise() %>%
mutate(p = apa_p_value(p),
petasq = as.numeric(as.character(petasq)),
petasq = ifelse(petasq < 0.01, "< .01", petasq),
result = paste("F(", DF1, ", ", DF2, ") = ", F, ", p ", p, ", eta2 = ", petasq, ", ", petasq_ci,sep = "")) %>%
ungroup()
# results for printing
m1_iv_result <- m1_output %>%
filter(Effect == "category") %>%
select(result) %>%
as.character()
m1_iv_result
plot_percentages(categorized_data)
plot_percentages_simple(categorized_data)
# exclusions: comments with less than 3 words
# dependencies
library(tidyverse)
library(tidytext)
library(tokenizers)
library(plotrix)  # for std.error
library(NLP)  # from github/ianhussey
plot_percentages_simple <- function(data) {
require(tidyverse)
require(plotrix)
data_summary <- data %>%
group_by(category) %>%
dplyr::summarize(mean  = mean(percent),
se = plotrix::std.error(percent))
plot <-
ggplot(data = data_summary,
aes(x = category, y = mean)) +
geom_pointrange(aes(ymax = mean + (1.96*se),
ymin = mean - (1.96*se)),
color = "black") +
ylab("Percentage of words") +
xlab("Category") +
scale_colour_grey() +
theme_classic() +
coord_flip()
return(plot)
}
# acquire data
input_data <- read.csv("../data acquisition and cleaning/data/5_data_for_analysis.csv") %>%
rename(parcel = comments,
id = identifier)
tidy_data <- tidy_parcels(data = input_data)
dictionary <- read.csv("lexicon_rfl.csv")
categorized_data <- categorize_parcels(data = tidy_data, dictionary = dictionary) %>%
mutate(category = as.factor(category))
dictionary <- read.csv("lexicon_rfl.csv")
categorized_data <- categorize_parcels(data = tidy_data, dictionary = dictionary) %>%
mutate(category = as.factor(category))
# exclusions: comments with less than 3 words
# dependencies
library(tidyverse)
library(tidytext)
library(tokenizers)
library(plotrix)  # for std.error
library(NLP)  # from github/ianhussey
plot_percentages_simple <- function(data) {
require(tidyverse)
require(plotrix)
data_summary <- data %>%
group_by(category) %>%
dplyr::summarize(mean  = mean(percent),
se = plotrix::std.error(percent))
plot <-
ggplot(data = data_summary,
aes(x = category, y = mean)) +
geom_pointrange(aes(ymax = mean + (1.96*se),
ymin = mean - (1.96*se)),
color = "black") +
ylab("Percentage of words") +
xlab("Category") +
scale_colour_grey() +
theme_classic() +
coord_flip()
return(plot)
}
# acquire data
input_data <- read.csv("../data acquisition and cleaning/data/5_data_for_analysis.csv") %>%
rename(parcel = comments,
id = identifier)
tidy_data <- tidy_parcels(data = input_data)
dictionary <- read.csv("lexicon_rfl.csv")
categorized_data <- categorize_parcels(data = tidy_data, dictionary = dictionary) %>%
mutate(category = as.factor(category))
category_booleans <- categorized_data %>%
group_by(parcel_number, category) %>%
summarize(bool = ifelse(sum(count) > 0, TRUE, FALSE)) %>%
ungroup() %>%
filter(category == "pets")
n <- category_booleans %>%
summarise(n = n()) %>%
as.integer()
pets <- category_booleans %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
as.integer()
pets/n
plot_percentages(categorized_data)
plot_percentages_simple(categorized_data)
library(ez)
library(MBESS)
library(schoRsch)
library(timesavers)
m1 <- ez::ezANOVA(data = categorized_data,
dv = percent,
within = category,
#between = NULL,
wid = parcel_number,
type = 2,
detailed = TRUE)
# tidy output
m1_summary <-
m1$ANOVA %>%
select(-SSn, -SSd, -`p<.05`, -ges) %>%
mutate(F = round(F, 2),
p = round(p, 3))
# partial etq squared
m1_petasq <-
anova_out(m1,
etasq = "partial",
print = FALSE)$`--- ANOVA RESULTS     ------------------------------------` %>%
select(Effect, petasq)
# CIs on partial eta squared
# "partial eta squared is the variance explained by a given variable of the variance remaining after excluding variance explained by other predictors."
# nb ci.pvaf throws an error for n when using within subjects designs. solution found at http://daniellakens.blogspot.be/2014/06/calculating-confidence-intervals-for.html
# CIs on main effect for intercept
lims <- conf.limits.ncf(F.value = m1_summary$`F`[1],
conf.level = 0.90,
df.1 = m1_summary$DFn[1],
df.2 = m1_summary$DFd[1])
lower.lim <- lims$Lower.Limit/(lims$Lower.Limit + m1_summary$DFn[1] + m1_summary$DFd[1] + 1)
upper.lim <- lims$Upper.Limit/(lims$Upper.Limit + m1_summary$DFn[1] + m1_summary$DFd[1] + 1)
ci_intercept <- paste("95% CI [",
round(lower.lim, 2),
", ",
round(upper.lim, 2),
"]",
sep = "")
# CIs on main effect
lims <- conf.limits.ncf(F.value = m1_summary$`F`[2],
conf.level = 0.90,
df.1 = m1_summary$DFn[2],
df.2 = m1_summary$DFd[2])
lower.lim <- lims$Lower.Limit/(lims$Lower.Limit + m1_summary$DFn[2] + m1_summary$DFd[2] + 1)
upper.lim <- lims$Upper.Limit/(lims$Upper.Limit + m1_summary$DFn[2] + m1_summary$DFd[2] + 1)
ci_iv <- paste("95% CI [",
round(lower.lim, 2),
", ",
round(upper.lim, 2),
"]",
sep = "")
# combine petasq values into df
m1_petasq_cis <- data.frame(Effect = c("(Intercept)",
"category"),
petasq_ci = c(ci_intercept,
ci_iv))
# combine all results into a single df
m1_output <- m1_summary %>%
left_join(m1_petasq, by = "Effect") %>%
left_join(m1_petasq_cis, by = "Effect") %>%
filter(Effect != "(Intercept)") %>%
dplyr::rename(DF1 = DFn,
DF2 = DFd) %>%
rowwise() %>%
mutate(p = apa_p_value(p),
petasq = as.numeric(as.character(petasq)),
petasq = ifelse(petasq < 0.01, "< .01", petasq),
result = paste("F(", DF1, ", ", DF2, ") = ", F, ", p ", p, ", eta2 = ", petasq, ", ", petasq_ci,sep = "")) %>%
ungroup()
# results for printing
m1_iv_result <- m1_output %>%
filter(Effect == "category") %>%
select(result) %>%
as.character()
library(ez)
library(MBESS)
library(schoRsch)
library(timesavers)
m1 <- ez::ezANOVA(data = categorized_data,
dv = percent,
within = category,
#between = NULL,
wid = parcel_number,
type = 2,
detailed = TRUE)
library(ez)
library(MBESS)
library(schoRsch)
library(timesavers)
m1 <- ez::ezANOVA(data = categorized_data,
dv = percent,
within = category,
#between = NULL,
wid = parcel_number,
type = 2,
detailed = TRUE)
# tidy output
m1_summary <-
m1$ANOVA %>%
select(-SSn, -SSd, -`p<.05`, -ges) %>%
mutate(F = round(F, 2),
p = round(p, 3))
# partial etq squared
m1_petasq <-
anova_out(m1,
etasq = "partial",
print = FALSE)$`--- ANOVA RESULTS     ------------------------------------` %>%
select(Effect, petasq)
# CIs on partial eta squared
# "partial eta squared is the variance explained by a given variable of the variance remaining after excluding variance explained by other predictors."
# nb ci.pvaf throws an error for n when using within subjects designs. solution found at http://daniellakens.blogspot.be/2014/06/calculating-confidence-intervals-for.html
# CIs on main effect for intercept
lims <- conf.limits.ncf(F.value = m1_summary$`F`[1],
conf.level = 0.90,
df.1 = m1_summary$DFn[1],
df.2 = m1_summary$DFd[1])
lower.lim <- lims$Lower.Limit/(lims$Lower.Limit + m1_summary$DFn[1] + m1_summary$DFd[1] + 1)
upper.lim <- lims$Upper.Limit/(lims$Upper.Limit + m1_summary$DFn[1] + m1_summary$DFd[1] + 1)
ci_intercept <- paste("95% CI [",
round(lower.lim, 2),
", ",
round(upper.lim, 2),
"]",
sep = "")
# CIs on main effect
lims <- conf.limits.ncf(F.value = m1_summary$`F`[2],
conf.level = 0.90,
df.1 = m1_summary$DFn[2],
df.2 = m1_summary$DFd[2])
lower.lim <- lims$Lower.Limit/(lims$Lower.Limit + m1_summary$DFn[2] + m1_summary$DFd[2] + 1)
upper.lim <- lims$Upper.Limit/(lims$Upper.Limit + m1_summary$DFn[2] + m1_summary$DFd[2] + 1)
ci_iv <- paste("95% CI [",
round(lower.lim, 2),
", ",
round(upper.lim, 2),
"]",
sep = "")
# combine petasq values into df
m1_petasq_cis <- data.frame(Effect = c("(Intercept)",
"category"),
petasq_ci = c(ci_intercept,
ci_iv))
# combine all results into a single df
m1_output <- m1_summary %>%
left_join(m1_petasq, by = "Effect") %>%
left_join(m1_petasq_cis, by = "Effect") %>%
filter(Effect != "(Intercept)") %>%
dplyr::rename(DF1 = DFn,
DF2 = DFd) %>%
rowwise() %>%
mutate(p = apa_p_value(p),
petasq = as.numeric(as.character(petasq)),
petasq = ifelse(petasq < 0.01, "< .01", petasq),
result = paste("F(", DF1, ", ", DF2, ") = ", F, ", p ", p, ", eta2 = ", petasq, ", ", petasq_ci,sep = "")) %>%
ungroup()
# results for printing
m1_iv_result <- m1_output %>%
filter(Effect == "category") %>%
select(result) %>%
as.character()
m1_iv_result
m1_iv_result
# remove stopwords
data("stop_words")  # from tidytext
stop_words_removed <- tidy_data %>%
anti_join(stop_words)
most_common_words <- stop_words_removed %>%
count(word, sort = TRUE) %>%
slice(1:100)
library(knitr)
most_common_words %>%
kable()
plot_percentages_simple(categorized_data)
category_booleans <- categorized_data %>%
group_by(parcel_number, category) %>%
summarize(bool = ifelse(sum(count) > 0, TRUE, FALSE)) %>%
ungroup()
View(category_booleans)
n <- category_booleans %>%
group_by(category) %>%
summarise(n = n()) %>%
as.integer()
n <- category_booleans %>%
group_by(category) %>%
summarise(n = n())
View(n)
n <- category_booleans %>%
summarise(n = n()) %>%
as.integer()
n <- category_booleans %>%
summarise(n = n()) %>%
slice(1) %>%
as.integer()
n <- category_booleans %>%
summarise(n = n()) %>%
slice(1) %>%
as.integer()
n <- category_booleans %>%
group_by(category) %>%
summarise(n = n()) %>%
slice(1) %>%
as.integer()
n
n <- category_booleans %>%
group_by(category) %>%
summarise(n = n()) %>%
slice(1)
summarise(n = n()) %>%
as.integer()
n <- category_booleans %>%
filter(category == "pets") %>%
summarise(n = n()) %>%
as.integer()
pets <- category_booleans %>%
group_by(category) %>%
filter(bool == TRUE)
View(pets)
pets <- category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n())
View(pets)
n_participants <- category_booleans %>%
filter(category == "pets") %>%
summarise(n = n()) %>%
as.integer()
pets <- category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants)
summary <- category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants)
View(summary)
summary <- category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants) %>%
round_df(2)
category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants) %>%
round_df(2) %>%
kable()
category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants) %>%
round_df(2) %>%
select(-n) %>%
kable()
# exclusions: comments with less than 3 words
# dependencies
library(tidyverse)
library(tidytext)
library(tokenizers)
library(plotrix)  # for std.error
library(NLP)  # from github/ianhussey
plot_percentages_simple <- function(data) {
require(tidyverse)
require(plotrix)
data_summary <- data %>%
group_by(category) %>%
dplyr::summarize(mean  = mean(percent),
se = plotrix::std.error(percent))
plot <-
ggplot(data = data_summary,
aes(x = category, y = mean)) +
geom_pointrange(aes(ymax = mean + (1.96*se),
ymin = mean - (1.96*se)),
color = "black") +
ylab("Percentage of words") +
xlab("Category") +
scale_colour_grey() +
theme_classic() +
coord_flip()
return(plot)
}
# acquire data
input_data <- read.csv("../data acquisition and cleaning/data/5_data_for_analysis.csv") %>%
rename(parcel = comments,
id = identifier)
tidy_data <- tidy_parcels(data = input_data)
dictionary <- read.csv("lexicon_rfl.csv")
categorized_data <- categorize_parcels(data = tidy_data, dictionary = dictionary) %>%
mutate(category = as.factor(category))
category_booleans <- categorized_data %>%
group_by(parcel_number, category) %>%
summarize(bool = ifelse(sum(count) > 0, TRUE, FALSE)) %>%
ungroup() %>%
filter(category == "pets")
n_participants <- category_booleans %>%
filter(category == "pets") %>%
summarise(n = n()) %>%
as.integer()
category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants) %>%
round_df(2) %>%
select(-n) %>%
kable()
category_booleans <- categorized_data %>%
group_by(parcel_number, category) %>%
summarize(bool = ifelse(sum(count) > 0, TRUE, FALSE)) %>%
ungroup() %>%
filter(category == "pets")
n_participants <- category_booleans %>%
filter(category == "pets") %>%
summarise(n = n()) %>%
as.integer()
category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants) %>%
round_df(2) %>%
select(-n) %>%
kable()
category_booleans <- categorized_data %>%
group_by(parcel_number, category) %>%
summarize(bool = ifelse(sum(count) > 0, TRUE, FALSE)) %>%
ungroup() %>%
filter(category == "pets")
View(category_booleans)
category_booleans <- categorized_data %>%
group_by(parcel_number, category) %>%
summarize(bool = ifelse(sum(count) > 0, TRUE, FALSE)) %>%
ungroup()
n_participants <- category_booleans %>%
filter(category == "pets") %>%
summarise(n = n()) %>%
as.integer()
category_booleans %>%
group_by(category) %>%
filter(bool == TRUE) %>%
summarise(n = n()) %>%
mutate(`Percent of comments` = n/n_participants) %>%
round_df(2) %>%
select(-n) %>%
kable()
library(tidy)
